The path to completion:

Continue working on DVMT documentation for WSDOT.

See plan-scenario.txt and scenarios.r for details on the new scenario architecture.

Elevate the priority of the multiple background process implementation.

Start testing VEModel with the future implementation
  1. sequential to start (should reproduce current live logging)
  2. multisession (and callr) to use some number of workers to do parallel processing
     Check the logic on all that

Things to do:
  A. Set up "packages" to load VEModel
  B. Set up "globals" (or "envir") to locate objects needed to evaluate the expression
  C. Run f <- future(...) and use value(f) to get the result

The above seems to work in a basic way. However, we don't want to run the models with dependency
on VEModel - we just want "visioneval" itself (the framework) - the expression to evaluate in the
future is the low-level sequence of model-running stuff.

Things to happen in the run expression for the stage:
  0. "self" is the VEModelStage object
  1. Change to self$RunPath
  2. Don't need to sweat the ve.model environment setup (it will be "new" for each process)
  3. Provide the "visioneval" package for the future
  4. Say something when the future is created
  5. After setting up all the futures, just return to the UI

From the UI, when we request the status of the model, it reports the RunStatus (which we
set to "Running" if the future exists but is not resolved, otherwise if the future exists then to
its value, otherwise if the future does not exist, just whatever is in RunStatus). If the future
exists and is resolved, then update the stage run status and delete the future.

So the "status" function gets a bit more complicated than just returning the number/status string.
It should check on the futures, and if it is newly resolved, it should reload the ModelState for the
stage.

Do we want a function to wait for the futures? With the scheme above, we never wait.

Do we want to reload the stage's ModelState while it might be running? Certainly we
want that after the fact.

When we run a stage, we stash a future object in the stage (initially NULL).
The model run function will create the futures for each stage in the current run group.
It will then loop over the stages looking for them to be complete/resolved. In sequential
or transparent resolution, each future will block until it is complete, so we need to log
"scheduling" or "processing" the stage (depending on the plan).

Might need to take control of the plan a bit more specifically...

Look closely at the multiple background process implementation for running scenarios in VEScenario.
That strategy seems to work well, but doesn't provide much of an interactive process controller...

Need to make sure the stage environment is set up suitably for stage to run - what is the minimal
enviroment we need - .libPaths(), VEModel loaded in the process, copy over any required setup in
the ve.model / ve.env environments that will get accessed. Ideally, we're as little subject to those
as possible.

What happens if we just wrap each stage in a "future" - do we need to re-open the model in that
environment, or can we pass it across?

It will probably be simpler to just push the RunParams_ls into the function to run in the background
and work at a low-level with framework functions, so the background process only uses the visioneval
package - we have that architectural separation now (VEModel just bottles up what the framework
needs to run in its new disaggregate fashion). models.R::1357-1377 (inside ve.stage.run).

We don't need the try-catch - that will be handled by the runner (trapping what comes back from the
"future").

Should we keep the runner structure (future, status, etc.) in the VEModelStage object? Then we
just iterate over the list of stages for status updates. Need to figure out how to manage that at
the VEModel level (running in one thread - the process manager) and then just check over the list
of model stages in the current run group for progress.

Could do a bigger structure for the run status (essentially, the runner object) that explains
the run status. So rather than just the RunStatus text, we have the whole apparatus of the future,
when the run was started, when it completed, etc. Pushing the run completion back into the model
state would be good too.

Report on stage groups:
  Group 0: Starting from Nothing
  Group 1: Starting from 'Some Stage in Group 0 or earlier'

The function should return ModelState_ls (after saving it) since we'll keep track of all those
for the background processes - let's see what the memory ramifications are for running hundreds
of scenario combinations.

The process running is all about model stages (not specifically "scenarios") - we'll use parallel
running for all stages that have the same "StartFrom" and set up BaseModel$setting("NumWorkers")
processes (possibly only 1) to run in parallel. Also use BaseModel$setting("ConcurrentPlan") to
map the name of a plan to the future function implementing the plan. That gets set prior to doing
a Model $run and controls how the stages will be processed. Default is "sequential" but can be
set to "transparent" or to one of the multi-process plans.

Process Management

Build a process controller for the model stages. To run the model, we create a queue of future
objects for each stage, with each one running a stage. +We can't start a stage until its "StartFrom"
is complete. So we'll group the stages by "StartFrom". We'll make a future running the stage into
each stage in the Group (we can start everybody in the group all at once); then we'll loop over
the group until all the elements are resolved.

Scan the list of stages to make groups:
  1. All stages, in order, with no "StartFrom"
  2. For each stage G in Group 1:
     All stages that StartFrom=="G"
  3. For each Group created in 2, do the same operation on any remaining ungrouped stages

The Groups are always run sequentially.
The individual stages in each group can run in parallel.
  If free workers > 0 add a future, otherwise poll ealier futures for resolved
    If resolved, then
      get the future's value (which should be the RunStatus) and put it in the Stage
      start the next future
    Otherwise, wait a second

The process runner can generate an offline status file as the processes it is polling finish and
new ones get started. That status file, which belongs to the model that the process runner is
processing, gets re-read when we ask for process status. That would ideally be a transactional
database... process runner does a polling loop on its processes to start N number of them, then
polls for finish and queue the next. It will also poll for presence of an updated request file. We
could just create a file system message queue (request/response) that uses files with standard
names and timestamps. So when we start running a model, we create the message queue directory (temp
directory) and send it to the process runner as a parameter. The process runner will poll for a
new file (written into under a temporary name, then renamed as an "almost atomic" operation) and
gather data to write into the response file (same timestamp as the request) - write the file under
a temp name, then rename it as an "almost atomic" operation. So we write the request, then go into
a loop waiting (up to a timeout) for the response file from the process runner.

Don't forget to manage the writeLog so when we do the run, the logging only goes to a file (not
the console).

Poll the r_process object for the run status. The results after the process is over (function
returns or fails) are backloaded into the model stage.

Need a simplified process status report (table) showing the Run Status for each stage, including
the low-level process status (so they can be cleaned up). Simple run management functions
(start/restart, stop, purge).

Create a "Waiting" status that is set into all the ModelStages for all the stages the model will
run. The ModelStages are grouped by their StartFrom (no StartFrom first). Then the process runner
will keep NUM_CORES processes running. When a process finishes, the next waiting in its group is set
in motion. When the group has all been set in motion and a core becomes free, the first waiting in
the next group is started. If any stage fails, the process runner does not add any additional
stages; all running ones will be allowed to complete/fail. Problems can be fixed, and a "continue"
run can be launched, which will set all incomplete stages to "Waiting" then start running them.

The process management does imply that we should have a single process runner that lives outside any
particular model. The model furnishes the list of stages to run and then calls the global
(singleton) process runner. That won't stop us from starting R twice, but it will let us trap
problems from having different models (or the same one!) running at the same time.

One way to manage that is not to allow any other model to run() until there are no more running
processes. Doing "running" will check if any model is running (it can be accessed via a model object
but also via a class function (VEModel$runner, for the process runner). That function will take a
command line option, and if none provided will show the running stages. We can ask for "waiting",
"complete", "failed", "running", or "paused" or some vector subset of those. We can also give a verb
plus PID to act on a particular process ("kill", "pause", "resume"). Default display: show
"complete", then "failed", then "waiting", then "running", then "paused". That is accomplished by
posing questions to the background process, which itself is in a loop to poll the processes.

Since the runner needs to be polling regularly (or using "future") to monitor the status of processes.

Keep stages waiting if their StartFrom is not "Run Complete", and when a stage finishes, find any
later stages that StartFrom it and set all of those running.
  Run up to "max_child" processes - a runtime configuration item in VEModel, defaulting to 4.
  When a stage finishes, see if there are more queued for the current StartFrom

We can efficiently run the stages like this: Classify them as those that start from scratch and
those that StartFrom. Note that if there is a LoadModel/LoadStage directive, that is treated like a
"StartFrom", and we look back to see if that model/stage is available and has been run, then copy it
into the first model stage. All of that should happen prior to starting parallel processing (though
we could run the LoadModel if it is not RunComplete).
  1. Collect the "free" stages and run them in parallel (probably there will only be one).
  3. When ANY stage is complete, look down the list of stages for those that StartFrom the completed
     one's name. Set all those running.
  2. Move to the first "StartFrom" stage, and find all the other stages that start from the same
  place; those can run in parallel
  3.

Experiment with running a stage using 'callr': what do we need to construct in the separate R process
environment: all data/configuration elements present in the same location (all relative to ModelDir).
The RunParam_ls for the stage should be complete - build it and pass it to the child process.

Absorb Dan et al's scripts for extracting metrics...

Create the model visualizer (dump detailed structures to a JSON file, and then launch a static page
to navigate it). Visualizer should include the scripts, the configurations (including grayed-in defaults),
the module inputs and outputs. That can be a warmup for the full scenario viewer.

Create an option to the addstage function to make a stage in the file system (sub-directory within
the ModelDir), containing an (optional) Scripts directory, a visioneval.cnf (with the extras, and an
Inputs directory)
  - Create=TRUE (by default): look for/create StageDir if any Scripts/Inputs/Config are specified;
    can be forced to FALSE to leave everything only in memory. If we don't specific at least one of
    Scripts/Inputs/Config, the stage will take those from the Model - still need the Scenario specifiers
    like Scenario, Description.
  - Scripts=TRUE (by default): look for/create ScriptsDir in StageDir
  - Inputs=TRUE (by default): look for/create InputDir in StageDir; NA is like TRUE in forcing Create but does not
    make an InputDir sub-directory (expect just to drop the inputs into StageDir)
  - Config=TRUE (or filename): dump stage-specific parameters into filename/visioneval.cnf (otherwise leave
    them only in the ModelStage structure saved into the model). If they get saved to a file, remove
    the extras from the ModelStage, leaving only the Name, Dir, Config, Reportable etc. basic parameters.

If inputs have been developed elsewhere, option to provide a string of file paths (absolute) that
will be copied into the scenario stage directory.

So the VEScenario operation should go like this:
  - Identify a Base Model (an open VEModel) and copy it to the Scenario Model
  - Identify the ScenarioConfig configuration files describing the scenarios (see VEScenario)
  - Identify (or create) the "StartFrom" stage that is the basis for all the scenarios (e.g. "Future Year")
  - Generate a (possibly long) sequence of Reportable model stages that get written back out to the
    Scenario Model Copy's visioneval.cnf by iterating through the Scenario configurations. This uses
    "addstage" and will create directories with visioneval.cnf and inputs for each combined scenario.
  - Extend that mechanism to handle manually constructed scenarios (see Eric's scripts) - that's a tweak
    to the configuration file.
  - Using this scenario function, the stages are written to the model's config (just Name and Dir),
    but we also create directories with visioneval.cnf plus inputs. Those directories will be mirrored
    in results.

The "addstage" function should default to rewriting the ModelStages structure in the model's
visioneval.cnf. Could just do it in memory (leaving the residue in the ModelState_ls/RunParam_ls
results structure).

Add a metadata element ("Notes" structure) to visioneval.cnf that just contains an array of strings.
Later useful in the model inspector.

Set up scenarios using addstage function:
  1. Add a scenario to the model's runtime
     - Start From (base scenario)
  2. Rewrite the model stages description to the Model/Stage configuration (save it back out)
     A. Model gets Stage Configuration (stage does not get explicit configuration)
     B. Stage Configuration:
        - Path may not be set (it includes "Inputs" and "Scripts" if those vary for the scenario)
        - Config is not set in this case (but could be)
        - StartFrom (base scenario that other scenarios modify)
        - Dir (name of output subfolder for this scenario in ResultsDir)
        - Name
     C. Extra configuration parameters (also placed in ModelStages)
        - InputPath (one or more directories with scenario inputs)
        - Description
        - Reportable (defaults to TRUE)

Update the scenario viewer to be fully table driven.

If we open a VisionEval result set, we want to be able to patch in missing pieces if the run_model.R
was run using "source" on a script that includes initialize model. Can we open a model that was
"source'd", or must it have run in VE? Probably the latter, though it might be interesting to see if
openModel/VEModel$new can inject the missing information into the ModelState (run status, etc) by
reading the model and inspecting the results. Or we could suspend certain types of error checking if
the result artifacts are present (i.e. assume that Run Status is complete if the directory contains
what is needed).

Make sure queries and results give better errors if deployed on a model that has not yet been run.

Do a walkthrough for staged models (with scenarios, going all the way through to queries).

Check on query geography adjustment - can that really work?
Check on using Bzones as the geography level.
Check on doing all geographies (or a subset) rather than a single GeoValue

Does the H5 storage format still work?

Extract should work on a flat Datastore - I think we're doing that by flattenig the DatastoreListing.

Restructure Query output to use sub-folders.

Automate the debug dump. If we have a model that has crashed in a certain stage, we should be
able to open it, then do a crash exploration.

Get RunScript specified and working (to go with RunModule).

VEModel workflow:

Do ALL the tests (re-run every single model) and save the log results in a file for scrutiny.

In VEQuery, look at Brian's Bzone adjustments and allow Bzones to be a summary unit.
Let the Geography value be a list for summarizing.

Longer term: consider give a class to the ModelState_ls as well, so printing it
  will just show its names (and perhaps a summary of each item - the text itself
  if it's short, or a summary of long, multi-element items)

It would be great to develop a "debug dump" tool:
  Copy and zip the full model plus ResultsDir into a temp dir
  Zip the temp dir
  Send to some large file recipient address (could use SLFTS to receive)
    These outputs can get truly huge!
  Then there's a setup for a new model using the provided one:
    Copy the crashed model to a new model directory
    Adjust run_model.R to pick up where it crashed (from Log)
    Use LoadModel/LoadStage to identify model and stage for LoadDatastore
      Important to set LoadStage since it may have crashed in an earlier stage
      that was not reportable. The default is the previous model's final stage.
    Rewrite the run_model.R and the visioneval.cnf

Here's a slightly more elaborate approach
Steps:
  0. Open a model
  1. User supplies names of packages and modules
     User supplies name for the resulting model slice (default 'PostMortem')
  2. Default is last package/module listed in the log file listed before the [Error]
     Option to bundle an entire runnable model (plus the Datastore)
  4. Pull out just the group/table/name for that listed module(s)'s "Get" specifications.
  5. Make a new Datastore with DatastoreListing.Rda in a PostMortem directory
     That should result from flattening the Datastore (but filtering its contents by
     Group/Table/Name). Can use the Datastore copy operation but with a specific
     list of Datastore elements (new implementation).
  6. Include the ModelState_ls and the log file; include the "defs" directory and an
     inputs directory that only includes the "Inp" files for the package/modules.

On the other side, have an openPostMortem function that will build a mini-model to run
the module that may have crashed.
  0. Expand out into a PostMortem model.
  1. Create a run_model.R and a visioneval.cnf (dump the ModelState_ls$RunParam_ls)
     Make sure the directories map into the PostMortem model... (adjust many RunParam_ls
     entries like ParamPath, etc.).
  2. Put the inputs and defs directories in place.
  3. Do LoadDatastore=TRUE and set the path to the PostMortem Datastore directory
     Does the Datastore name have to include getRunParameter("DatastoreName")?
  4. Should have a runnable model with one stage that loads the PostMortem datastore
     and runs just the modules that were used to generate the PostMortem.

PostMortem need not be run on a failed model - it can be used to create a testbed for
any module, even if one that is working correctly.

Likewise with queries: resutsdir/outputs/query-timestamp/{queryspec,scenarios (results paths), output table}

Review Arash's scenario manager plus Eric Englin's additions.

#) Make several copies of the same base model with different years (only future) and different
   Model Name and Scenario Name.  Then check that we get good output from:
     List of model names
     List of model results
     List of model result directories
#) Add some specifications with the "By" option (income analysis, also by MArea or Azone, and by
   both. See how that shows up in the data.frame of results
#) Test running the same query set at different geographies
#) Think about the API for looking into a "scenario" root directory (where we might probe into
   sub-folders looking for VEResults based on existence of Datastore and ModelState.rda...).
   Eventually all that gets easier with the new VEScenario approach - we'll be able to require
   everything to have the same BaseModel, and a single master ResultsDir for the scenarios.

#) Explore running models in a separate space
   a) Launch an R process that we pre-load with objects from the current process
      Need visioneval, VEModel (full .libPaths); set working directory (ve.runtime)
      and then load/run a particular model. Need to make sure that the whole search
      path is properly constructed (including local environments) and that the
      runtime environment stuff stays loaded. What's the least we could do in a child
      process to make a model run (load VEModel, open the model, run it). Then when
      the process is done, reload the model state (as we currently do) in the front-end
      R session.
#) Get the scenario stuff integrated - very simple set of scenarios
   a) That will be the moment to change the Datastore access, which will need some
      careful thinking about how to initialize (bomb the model initialization if
      the base model has not been run, or should we recursively run the base model?).
      Specify the base model as a VEModel, or just by locating its ResultsDir (both).
   b) Future scenarios just run individual years, not the BaseYear. If there is no BaseModel
      run the BaseYear. Otherwise just run the other explicit years. Models with a BaseModel
      can but should not run the BaseYear.
   c) The key for the scenarios is not to run the BaseYear, which we can easily do just by
      leaving it out of Years.
   d) The BaseModel stuff initially should also encompass the RunScript, and load the
      BaseModel run parameters (so the derived model doesn't need to have any configuration
      other than the InputPath and ResultsDir). If we define Scenarios within the BaseModel
      then we'll just need a set of InputPath elements for each scenario category/level
      that we concatenate into a full set for the Scenario. So the tree could look like:
      /Base-Model
        /inputs
        /defs
        /results
        /queries
        /Scenarios
          /ScenarioName
            ... config files at the root
            ... config files specify category/level plus names
            ... config files also specify the .VEqry that will be applied to generate
                a comparative table of all the scenario results.
            /inputs
              /Category-Level folders with the input files
            /results
              /One subfolder for each permutation of category/level
              /outputs (for queries that run across the full set of results)
   e) Then do the visualizer base on what appears in the 

** Inspector

The inspectModel function should do a very simple interaction:
  1) We launch the HTML viewer, and point it at the page for the kind
     of thing being inspected
  2) Should be able to walk up or down the ladder
  3) Pass to Javascript should be a "Model" or "Collection" (Backbone concepts) with
     a particular name/processing type
  4) Stuff is available

Things we want to inspect:
  1) Settings
    a) Defaults
    b) Global (after ve.runtime)
  2) Models (model directory)
    a) List all models and inspect one
  3) Queries (query directory)
    a) List all queries (root) and inspect one
  4) One Model
    a) List all model stages and inspect one
    b) List all queries for Model (global, model-specific) and inspect one
    c) List Identifier, paths to all result sets for the Model
  5) One Model Stage
    a) Settings
    b) Input Directory (and files present)
    c) Param Dir (and files present)
    d) run_model.R script (raw text)
    e) initializeModel parameters (LoadDatastore)
    f) AllSpecs_ls (ordered sequence of Package/Module/Specs)
        Show Packages
        Within Packages expand to show Modules
        Within Modules expand to show Input, Get, Set specs
        Within a spec
          If Input, show InputDir, File, Group, Table
            Within InputDir,File show Fields, Units, Description
              Expand optionally to remining non-NA spec elements
          If Get/Set
            Show Group/Table
              Within Group, Table show Name, Units, Description
                Expand optionally to remaining non-NA spec elements
  6) One Query
    a) List of Query Specifications (names) and expression, pick one
    b) One Query Specification
       Full list of defined specification elements
