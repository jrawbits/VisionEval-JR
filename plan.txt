The path to completion:

Continue working on DVMT documentation for WSDOT.

See plan-scenario.txt and scenarios.r for details on the new scenario architecture.

Start testing VEModel with the future implementation
  1. sequential is mapped over to "inline" which runs the model in the interactive R
     session (so you can watch the logfile "live"). Seems to be working. Blocks while
     each stage is running.
  2. multisession (or callr) can use some number of workers to do parallel processing
     Basically working, but the internal plan handling needs to have an option for
     number of workers (currently uses all available). Make those options part of the
     RunParam_ls options? Main process becomes a controller, with status updates on
     how long each stage has been running. The log file for the last stage in each group
     can be polled for updates (so you can see it finish "live").

Start to develop the stage/scenario walkthrough (will furnish tests for scenario
elements).

Do a multi-stage model (set up a model that has "parallelizable" stages so we can see
the parallel run working). We'll get a lot more parallelism going as scenarios emerge.

Doing a very basic implementation using the "future" package.

Absorb Dan et al's scripts for extracting metrics...

Create the model visualizer (dump detailed structures to a JSON file, and then launch a static page
to navigate it). Visualizer should include the scripts, the configurations (including grayed-in defaults),
the module inputs and outputs. That can be a warmup for the full scenario viewer.

Create an option to the addstage function to make a stage in the file system (sub-directory within
the ModelDir), containing an (optional) Scripts directory, a visioneval.cnf (with the extras, and an
Inputs directory)
  - Create=TRUE (by default): look for/create StageDir if any Scripts/Inputs/Config are specified;
    can be forced to FALSE to leave everything only in memory. If we don't specific at least one of
    Scripts/Inputs/Config, the stage will take those from the Model - still need the Scenario specifiers
    like Scenario, Description.
  - Scripts=TRUE (by default): look for/create ScriptsDir in StageDir
  - Inputs=TRUE (by default): look for/create InputDir in StageDir; NA is like TRUE in forcing Create but does not
    make an InputDir sub-directory (expect just to drop the inputs into StageDir)
  - Config=TRUE (or filename): dump stage-specific parameters into filename/visioneval.cnf (otherwise leave
    them only in the ModelStage structure saved into the model). If they get saved to a file, remove
    the extras from the ModelStage, leaving only the Name, Dir, Config, Reportable etc. basic parameters.

If inputs have been developed elsewhere, option to provide a string of file paths (absolute) that
will be copied into the scenario stage directory.

So the VEScenario operation should go like this:
  - Identify a Base Model (an open VEModel) and copy it to the Scenario Model
  - Identify the ScenarioConfig configuration files describing the scenarios (see VEScenario)
  - Identify (or create) the "StartFrom" stage that is the basis for all the scenarios (e.g. "Future Year")
  - Generate a (possibly long) sequence of Reportable model stages that get written back out to the
    Scenario Model Copy's visioneval.cnf by iterating through the Scenario configurations. This uses
    "addstage" and will create directories with visioneval.cnf and inputs for each combined scenario.
  - Extend that mechanism to handle manually constructed scenarios (see Eric's scripts) - that's a tweak
    to the configuration file.
  - Using this scenario function, the stages are written to the model's config (just Name and Dir),
    but we also create directories with visioneval.cnf plus inputs. Those directories will be mirrored
    in results.

The "addstage" function should default to rewriting the ModelStages structure in the model's
visioneval.cnf. Could just do it in memory (leaving the residue in the ModelState_ls/RunParam_ls
results structure).

Add a metadata element ("Notes" structure) to visioneval.cnf that just contains an array of strings.
Later useful in the model inspector.

Set up scenarios using addstage function:
  1. Add a scenario to the model's runtime
     - Start From (base scenario)
  2. Rewrite the model stages description to the Model/Stage configuration (save it back out)
     A. Model gets Stage Configuration (stage does not get explicit configuration)
     B. Stage Configuration:
        - Path may not be set (it includes "Inputs" and "Scripts" if those vary for the scenario)
        - Config is not set in this case (but could be)
        - StartFrom (base scenario that other scenarios modify)
        - Dir (name of output subfolder for this scenario in ResultsDir)
        - Name
     C. Extra configuration parameters (also placed in ModelStages)
        - InputPath (one or more directories with scenario inputs)
        - Description
        - Reportable (defaults to TRUE)

Update the scenario viewer to be fully table driven.

If we open a VisionEval result set, we want to be able to patch in missing pieces if the run_model.R
was run using "source" on a script that includes initialize model. Can we open a model that was
"source'd", or must it have run in VE? Probably the latter, though it might be interesting to see if
openModel/VEModel$new can inject the missing information into the ModelState (run status, etc) by
reading the model and inspecting the results. Or we could suspend certain types of error checking if
the result artifacts are present (i.e. assume that Run Status is complete if the directory contains
what is needed).

Make sure queries and results give better errors if deployed on a model that has not yet been run.

Do a walkthrough for staged models (with scenarios, going all the way through to queries).

Check on query geography adjustment - can that really work?
Check on using Bzones as the geography level.
Check on doing all geographies (or a subset) rather than a single GeoValue

Does the H5 storage format still work?

Extract should work on a flat Datastore - I think we're doing that by flattenig the DatastoreListing.

Restructure Query output to use sub-folders.

Automate the debug dump. If we have a model that has crashed in a certain stage, we should be
able to open it, then do a crash exploration.

Get RunScript specified and working (to go with RunModule).

VEModel workflow:

Do ALL the tests (re-run every single model) and save the log results in a file for scrutiny.

In VEQuery, look at Brian's Bzone adjustments and allow Bzones to be a summary unit.
Let the Geography value be a list for summarizing.

Longer term: consider give a class to the ModelState_ls as well, so printing it
  will just show its names (and perhaps a summary of each item - the text itself
  if it's short, or a summary of long, multi-element items)

It would be great to develop a "debug dump" tool:
  Copy and zip the full model plus ResultsDir into a temp dir
  Zip the temp dir
  Send to some large file recipient address (could use SLFTS to receive)
    These outputs can get truly huge!
  Then there's a setup for a new model using the provided one:
    Copy the crashed model to a new model directory
    Adjust run_model.R to pick up where it crashed (from Log)
    Use LoadModel/LoadStage to identify model and stage for LoadDatastore
      Important to set LoadStage since it may have crashed in an earlier stage
      that was not reportable. The default is the previous model's final stage.
    Rewrite the run_model.R and the visioneval.cnf

Here's a slightly more elaborate approach
Steps:
  0. Open a model
  1. User supplies names of packages and modules
     User supplies name for the resulting model slice (default 'PostMortem')
  2. Default is last package/module listed in the log file listed before the [Error]
     Option to bundle an entire runnable model (plus the Datastore)
  4. Pull out just the group/table/name for that listed module(s)'s "Get" specifications.
  5. Make a new Datastore with DatastoreListing.Rda in a PostMortem directory
     That should result from flattening the Datastore (but filtering its contents by
     Group/Table/Name). Can use the Datastore copy operation but with a specific
     list of Datastore elements (new implementation).
  6. Include the ModelState_ls and the log file; include the "defs" directory and an
     inputs directory that only includes the "Inp" files for the package/modules.

On the other side, have an openPostMortem function that will build a mini-model to run
the module that may have crashed.
  0. Expand out into a PostMortem model.
  1. Create a run_model.R and a visioneval.cnf (dump the ModelState_ls$RunParam_ls)
     Make sure the directories map into the PostMortem model... (adjust many RunParam_ls
     entries like ParamPath, etc.).
  2. Put the inputs and defs directories in place.
  3. Do LoadDatastore=TRUE and set the path to the PostMortem Datastore directory
     Does the Datastore name have to include getRunParameter("DatastoreName")?
  4. Should have a runnable model with one stage that loads the PostMortem datastore
     and runs just the modules that were used to generate the PostMortem.

PostMortem need not be run on a failed model - it can be used to create a testbed for
any module, even if one that is working correctly.

Likewise with queries: resutsdir/outputs/query-timestamp/{queryspec,scenarios (results paths), output table}

Review Arash's scenario manager plus Eric Englin's additions.

#) Make several copies of the same base model with different years (only future) and different
   Model Name and Scenario Name.  Then check that we get good output from:
     List of model names
     List of model results
     List of model result directories
#) Add some specifications with the "By" option (income analysis, also by MArea or Azone, and by
   both. See how that shows up in the data.frame of results
#) Test running the same query set at different geographies
#) Think about the API for looking into a "scenario" root directory (where we might probe into
   sub-folders looking for VEResults based on existence of Datastore and ModelState.rda...).
   Eventually all that gets easier with the new VEScenario approach - we'll be able to require
   everything to have the same BaseModel, and a single master ResultsDir for the scenarios.

#) Explore running models in a separate space
   a) Launch an R process that we pre-load with objects from the current process
      Need visioneval, VEModel (full .libPaths); set working directory (ve.runtime)
      and then load/run a particular model. Need to make sure that the whole search
      path is properly constructed (including local environments) and that the
      runtime environment stuff stays loaded. What's the least we could do in a child
      process to make a model run (load VEModel, open the model, run it). Then when
      the process is done, reload the model state (as we currently do) in the front-end
      R session.
#) Get the scenario stuff integrated - very simple set of scenarios
   a) That will be the moment to change the Datastore access, which will need some
      careful thinking about how to initialize (bomb the model initialization if
      the base model has not been run, or should we recursively run the base model?).
      Specify the base model as a VEModel, or just by locating its ResultsDir (both).
   b) Future scenarios just run individual years, not the BaseYear. If there is no BaseModel
      run the BaseYear. Otherwise just run the other explicit years. Models with a BaseModel
      can but should not run the BaseYear.
   c) The key for the scenarios is not to run the BaseYear, which we can easily do just by
      leaving it out of Years.
   d) The BaseModel stuff initially should also encompass the RunScript, and load the
      BaseModel run parameters (so the derived model doesn't need to have any configuration
      other than the InputPath and ResultsDir). If we define Scenarios within the BaseModel
      then we'll just need a set of InputPath elements for each scenario category/level
      that we concatenate into a full set for the Scenario. So the tree could look like:
      /Base-Model
        /inputs
        /defs
        /results
        /queries
        /Scenarios
          /ScenarioName
            ... config files at the root
            ... config files specify category/level plus names
            ... config files also specify the .VEqry that will be applied to generate
                a comparative table of all the scenario results.
            /inputs
              /Category-Level folders with the input files
            /results
              /One subfolder for each permutation of category/level
              /outputs (for queries that run across the full set of results)
   e) Then do the visualizer base on what appears in the 

** Inspector

The inspectModel function should do a very simple interaction:
  1) We launch the HTML viewer, and point it at the page for the kind
     of thing being inspected
  2) Should be able to walk up or down the ladder
  3) Pass to Javascript should be a "Model" or "Collection" (Backbone concepts) with
     a particular name/processing type
  4) Stuff is available

Things we want to inspect:
  1) Settings
    a) Defaults
    b) Global (after ve.runtime)
  2) Models (model directory)
    a) List all models and inspect one
  3) Queries (query directory)
    a) List all queries (root) and inspect one
  4) One Model
    a) List all model stages and inspect one
    b) List all queries for Model (global, model-specific) and inspect one
    c) List Identifier, paths to all result sets for the Model
  5) One Model Stage
    a) Settings
    b) Input Directory (and files present)
    c) Param Dir (and files present)
    d) run_model.R script (raw text)
    e) initializeModel parameters (LoadDatastore)
    f) AllSpecs_ls (ordered sequence of Package/Module/Specs)
        Show Packages
        Within Packages expand to show Modules
        Within Modules expand to show Input, Get, Set specs
        Within a spec
          If Input, show InputDir, File, Group, Table
            Within InputDir,File show Fields, Units, Description
              Expand optionally to remining non-NA spec elements
          If Get/Set
            Show Group/Table
              Within Group, Table show Name, Units, Description
                Expand optionally to remaining non-NA spec elements
  6) One Query
    a) List of Query Specifications (names) and expression, pick one
    b) One Query Specification
       Full list of defined specification elements
