The path to completion:

Fix up the NextGen runtime installer: use Setup.R from the walkthrough to locate ve-lib (with some
additional installation stuff that might work on the Mac from the original installer). Basically,
all we need in the runtime is something that will locate ve-lib (or build from ve-pkg), then require
VEModel. Create the models folder but do not include it. Keep the tools folder mechanism for testing
out future enhancements. Do not (at this time) include the VEScenarioViewer.

Create the model visualizer (dump detailed structures to a JSON file, and then launch a static page
to navigate it). Visualizer should include the scripts, the configurations (including grayed-in defaults),
the module inputs and outputs. That can be a warmup for the full scenario viewer.

Update the scenario viewer to be fully table driven.

If we open a VisionEval result set, we want to be able to patch in missing pieces if the run_model.R
was run using "source" on a script that includes initialize model. Can we open a model that was
"source'd", or must it have run in VE? Probably the latter, though it might be interesting to see if
openModel/VEModel$new can inject the missing information into the ModelState (run status, etc) by
reading the model and inspecting the results. Or we could suspend certain types of error checking if
the result artifacts are present (i.e. assume that Run Status is complete if the directory contains
what is needed).

Make sure queries and results give better errors if deployed on a model that has not yet been run.

Do a walkthrough for staged models (with scenarios, going all the way through to queries).

Check on query geography adjustment - can that really work?
Check on using Bzones as the geography level.
Check on doing all geographies (or a subset) rather than a single GeoValue

Does the H5 storage format still work?

Extract should work on a flat Datastore - I think we're doing that by flattenig the DatastoreListing.

Restructure Query output to use sub-folders.

Automate the debug dump. If we have a model that has crashed in a certain stage, we should be
able to open it, then do a crash exploration.

Get RunScript specified and working (to go with RunModule).

VEModel workflow:

Do ALL the tests (re-run every single model) and save the log results in a file for scrutiny.

In VEQuery, look at Brian's Bzone adjustments and allow Bzones to be a summary unit.
Let the Geography value be a list for summarizing.

Longer term: consider give a class to the ModelState_ls as well, so printing it
  will just show its names (and perhaps a summary of each item - the text itself
  if it's short, or a summary of long, multi-element items)

It would be great to develop a "debug dump" tool:
  Copy and zip the full model plus ResultsDir into a temp dir
  Zip the temp dir
  Send to some large file recipient address (could use SLFTS to receive)
    These outputs can get truly huge!
  Then there's a setup for a new model using the provided one:
    Copy the crashed model to a new model directory
    Adjust run_model.R to pick up where it crashed (from Log)
    Use LoadModel/LoadStage to identify model and stage for LoadDatastore
      Important to set LoadStage since it may have crashed in an earlier stage
      that was not reportable. The default is the previous model's final stage.
    Rewrite the run_model.R and the visioneval.cnf

Here's a slightly more elaborate approach
Steps:
  0. Open a model
  1. User supplies names of packages and modules
     User supplies name for the resulting model slice (default 'PostMortem')
  2. Default is last package/module listed in the log file listed before the [Error]
     Option to bundle an entire runnable model (plus the Datastore)
  4. Pull out just the group/table/name for that listed module(s)'s "Get" specifications.
  5. Make a new Datastore with DatastoreListing.Rda in a PostMortem directory
     That should result from flattening the Datastore (but filtering its contents by
     Group/Table/Name). Can use the Datastore copy operation but with a specific
     list of Datastore elements (new implementation).
  6. Include the ModelState_ls and the log file; include the "defs" directory and an
     inputs directory that only includes the "Inp" files for the package/modules.

On the other side, have an openPostMortem function that will build a mini-model to run
the module that may have crashed.
  0. Expand out into a PostMortem model.
  1. Create a run_model.R and a visioneval.cnf (dump the ModelState_ls$RunParam_ls)
     Make sure the directories map into the PostMortem model... (adjust many RunParam_ls
     entries like ParamPath, etc.).
  2. Put the inputs and defs directories in place.
  3. Do LoadDatastore=TRUE and set the path to the PostMortem Datastore directory
     Does the Datastore name have to include getRunParameter("DatastoreName")?
  4. Should have a runnable model with one stage that loads the PostMortem datastore
     and runs just the modules that were used to generate the PostMortem.

PostMortem need not be run on a failed model - it can be used to create a testbed for
any module, even if one that is working correctly.

Likewise with queries: resutsdir/outputs/query-timestamp/{queryspec,scenarios (results paths), output table}

Review Arash's scenario manager plus Eric Englin's additions.

#) Make several copies of the same base model with different years (only future) and different
   Model Name and Scenario Name.  Then check that we get good output from:
     List of model names
     List of model results
     List of model result directories
#) Add some specifications with the "By" option (income analysis, also by MArea or Azone, and by
   both. See how that shows up in the data.frame of results
#) Test running the same query set at different geographies
#) Think about the API for looking into a "scenario" root directory (where we might probe into
   sub-folders looking for VEResults based on existence of Datastore and ModelState.rda...).
   Eventually all that gets easier with the new VEScenario approach - we'll be able to require
   everything to have the same BaseModel, and a single master ResultsDir for the scenarios.

#) Explore running models in a separate space
   a) Launch an R process that we pre-load with objects from the current process
      Need visioneval, VEModel (full .libPaths); set working directory (ve.runtime)
      and then load/run a particular model. Need to make sure that the whole search
      path is properly constructed (including local environments) and that the
      runtime environment stuff stays loaded. What's the least we could do in a child
      process to make a model run (load VEModel, open the model, run it). Then when
      the process is done, reload the model state (as we currently do) in the front-end
      R session.
#) Get the scenario stuff integrated - very simple set of scenarios
   a) That will be the moment to change the Datastore access, which will need some
      careful thinking about how to initialize (bomb the model initialization if
      the base model has not been run, or should we recursively run the base model?).
      Specify the base model as a VEModel, or just by locating its ResultsDir (both).
   b) Future scenarios just run individual years, not the BaseYear. If there is no BaseModel
      run the BaseYear. Otherwise just run the other explicit years. Models with a BaseModel
      can but should not run the BaseYear.
   c) The key for the scenarios is not to run the BaseYear, which we can easily do just by
      leaving it out of Years.
   d) The BaseModel stuff initially should also encompass the RunScript, and load the
      BaseModel run parameters (so the derived model doesn't need to have any configuration
      other than the InputPath and ResultsDir). If we define Scenarios within the BaseModel
      then we'll just need a set of InputPath elements for each scenario category/level
      that we concatenate into a full set for the Scenario. So the tree could look like:
      /Base-Model
        /inputs
        /defs
        /results
        /queries
        /Scenarios
          /ScenarioName
            ... config files at the root
            ... config files specify category/level plus names
            ... config files also specify the .VEqry that will be applied to generate
                a comparative table of all the scenario results.
            /inputs
              /Category-Level folders with the input files
            /results
              /One subfolder for each permutation of category/level
              /outputs (for queries that run across the full set of results)
   e) Then do the visualizer base on what appears in the 

** Inspector

The inspectModel function should do a very simple interaction:
  1) We launch the HTML viewer, and point it at the page for the kind
     of thing being inspected
  2) Should be able to walk up or down the ladder
  3) Pass to Javascript should be a "Model" or "Collection" (Backbone concepts) with
     a particular name/processing type
  4) Stuff is available

Things we want to inspect:
  1) Settings
    a) Defaults
    b) Global (after ve.runtime)
  2) Models (model directory)
    a) List all models and inspect one
  3) Queries (query directory)
    a) List all queries (root) and inspect one
  4) One Model
    a) List all model stages and inspect one
    b) List all queries for Model (global, model-specific) and inspect one
    c) List Identifier, paths to all result sets for the Model
  5) One Model Stage
    a) Settings
    b) Input Directory (and files present)
    c) Param Dir (and files present)
    d) run_model.R script (raw text)
    e) initializeModel parameters (LoadDatastore)
    f) AllSpecs_ls (ordered sequence of Package/Module/Specs)
        Show Packages
        Within Packages expand to show Modules
        Within Modules expand to show Input, Get, Set specs
        Within a spec
          If Input, show InputDir, File, Group, Table
            Within InputDir,File show Fields, Units, Description
              Expand optionally to remining non-NA spec elements
          If Get/Set
            Show Group/Table
              Within Group, Table show Name, Units, Description
                Expand optionally to remaining non-NA spec elements
  6) One Query
    a) List of Query Specifications (names) and expression, pick one
    b) One Query Specification
       Full list of defined specification elements
