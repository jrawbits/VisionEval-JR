The Partitioning plan in brief:

TODO: 8/18/2023

Hanging up extracting model results. There's a mismatch in the base test model between
the name attached to the Results in the VEResultsList and the name of the scenario
selected for investigation - is the wrong Scenario name going into the results list?


TODO: 8/16/2023

In developing the TableString, we need to track names of the Paths and Names elements in the
TableLocator, not just the values (i.e. Global=Global, Marea=RVMPO). Otherwise we'll get
collisions between Azone=RVMPO and Marea=RVMPO if we decide to do that level of partition.
So in makeing the string, we'll "Name=Value" for Paths and Names (and in the partitioning,
need to include the names), and when we unpack the string, need to look for "=". Means we
always need the TableLocator to interpret what table we're looking at.
Alternatively, could just quietly presume we won't partition the same
Table on different fields that might have the same values. But maybe it's not an issue

Consider SQL/SQLite with dbplyr for large database manipulation in R as an alternative to $data
or perhaps a format option for it on the DBI interface... Could just do this as an optional return
format for tables in $data:

  tableList <- lapply( dbTableNames, function(nm) dplyr::tbl(self$dbConnection,nm) )
versus the default:
  tableList <- lapply( dbTableNames, function(nm) DBI::readTable(self$dbConnection,nm) )

and of course, with an appropiate allocator, could also load up dataTables, tibbles, etc.

The dbplyr option wants us to save/load an existing exporter: all we need are the
connection details to re-open the connection, plus the list of TableLocators. The metadata
is itself just a locatable table...

TODO: 8/15/2023

Done with first cut at implement VEPartition.

Set up basic framework for VEExporter and the connections

Next up:

 * Finished setting up the basic tests.
 * Finished basic metadata implementation (need to check what gets created when we open the
   results list).
 * Working on exporter

 * implement the data.frame connection and see if we can partition by different features. It might
   be easier just to start with the CSV implementation (we'll end up iterating through a bunch of
   data.frames, which is not impossible, but will need a fair amount of machinery).

 * Then we should be able to run pretty quickly... Play with different partition schemes.

 * Move on to doing the $list and $data functions (list of tables, option for full metadata)
   $data should get the list of tables (in connection-specific lookups), open and read them
   one by one using the connection and generate a mammoth list of data.frames.


TODO: 8/14/2023

Notes while implementing:

Don't rush to implement the re-partitioning scheme (need to consult the Metadata for the copied
Exporter).

Don't rush to implement the save/load exporter either. For now, we'll just spin them up from the
Datastore each time.

Don't rush to implement the selections. For now, we'll just select everything (the complete
field list). Later doing the subset with searches on names should be very straightforward.

Implement VEPartition as an object that can take a data.frame and partition it into sets of rows
according to the partition scheme and what's in the data.frame (returns a control list of physical
table locators indicating range of rows to select from the data.frame). So we can have two elements:
the Range (a vector of data.frame row indices), the TableLoc, and the Metadata for the Table implied
by TableLoc (S/G/T/N + N's Metadata columns, plus formatted table name). For the data.frame
connection, formatted table name is a pre-formatted set of R index code that will select out nested
Scenario/Group/Table depending on the folder partitioning. Like "['Scenario-']['2045']['Household']"
The Metadata piece from the TableLoc is what gets written to the Metadata table in the connection.

Implement VEConnection as a thing that presents an interface to Create/Append/Read data given a
data.frame (for Create/Append, subsetted using the $partition Range element) and a partition table
spec (The TableLoc, with path, folders, table, and Metadata) as is returned from VEPartition.
Returned from Create/Write is the Metadata with the DBTable column added, containing a spec usable
to find the table later (e.g. if we open the Metadata $list for the exporter and look up an S/G/T/N
and pull out DBTable to get the actual data - used in the $data function, just by iterating through
unique DBTables).

Other VEConnection pieces include saving TablePrefix, TableSuffix, and obviously whatever internal
low-level communication info is needed for the individual types.

It will implement these functions:
  initialize from a Driver + Connection
    can handle TablePrefix and TableSuffix
  driver can be specified with a "tag" for standard known drivers (csv, sql) or anything
    else we care to create (specify in visioneval.cnf under Exporters: Tag:)
  we can use an existing connection (and just reconnect to it, for use in a new exporter to the same place)
  initialize parameters:
    tag (optional - pull missing configuration parameters from there)
    TablePrefix
    TableSuffix (default here can be timestamp)
    ... which has things like:
      rootFolder (for CSV or SQLite type)
      con (for DBI - an existing DBI connection created outside)
      dbDriver is a function we can call (probably only relevant for DBI connection type)
  Still want to mediate that via openExporter to get the right Connection Class.
  The openExporter / VEExporter connection parameter is something that has all the elements except
    "tag" (tag selects the class of VEConnection; connection list is passed to it to supplant the
    defaults). "tag" is opened first, reaching for system defaults (built in) if not overwritten.
    Then the connection list is used via do.call to the constructor adding in the defaults for the
    tag if any, then the items from visioneval.cnf (Exporter:Tag) and then the items in the list
    passed here.

  Any of those can be passed in via ... if not provided explicitly
  makeTableName from a TableLoc (from VEPartition)
  listTables on the connection (from its internal list of what has been written)
  dropTable (also removes the tablename from listTables)
  writeTable (dropping if 'overwrite=TRUE', creating if non-existent or appending; use a TableLoc to identify which table)
  readTable(given a TableLoc or an element returned from listTables; fail if no table)

DataFrame:
  Interprets TableLoc per the partition into folders (lists) containing other folders containing Tables
  makeTableName produces an index list into the dimensions
CSV:
  OutputFolder = results/outputs + Model Results Folder where tables are actually created
    (constructor builds from RootFolder + (ModelName,Timestamp))
  Table filenames concatenated from Name elements plus Table
  Full Table Path is stored back into Metadata (with file.path separators) relative to RootFolder
SQLite:
  RootFolder = results/outputs
  Database File (constructor builds from RootFolder + SQLite database template(ModelName,Timestamp))
  Just wraps the more general DBI Connection by defaulting the table name path and using the
  RSQLite::SQLite() driver.
  Table names inside the database file concatenated from Path and Name elements plus Table
  Full Table Path is stored back into 
DBI/SQL:
  Database connection (arbitrary, per DBI Rules). Essentially like this:
    con <- dbConnect(RSQLite::SQLite(), ":memory:)
  Table names inside the database concatenated from Path and Name elements
  Track Driver and Connection (Connection is a possibly empty list of parameters passed to dbCnnect)
  Full Table Path is the connection description (Driver=,Connection=) and TableName

Get the Partition and basic exporter working
Create a test function for exporting specifically (not just within results testing, but could be)
Run all the results tests (since the results got reorganized).
Test out export to:
  - Data.frame
  - CSV
  - Sqlite (via DBI)
Make sure connection configuration via visioneval.cnf works (at model or global or default level).

===== NOTES ON IMPLEMENTATION

Could we just have one Exporter class and push the details of different types of storage down into
the connection? The main question would be saving the list of tables generated as the tables are
written out - that requires cooperation with the connection. We can explore and refactor later.
The factory class pretty much hides the details - we'll get the VEExporter interface in any case
and if we later re-delegate to the conneciton within a single class it should be transparent.

Partition scheme is a named character vector with fields to partition on:
  c(Scenario="folder",Group="merge") # Don't need to include Table as it will always be "name"

"database" partition will attempt to write to a database tagged with the given name.
   So we could stuff "Global" stuff into a different database
   Connection can have multiple databases identified by name ( partition: (Database="altDB" which
   is then applied to construct the Table in that database with whatever other partitioning).
   Throw an error if Database is specified in the write and not found in the connection (or perhaps
   create the first database as the "default").

"merge" or "none" partition makes explicit the default, which is to ignore that field in picking out
  the Table.
"folder" partition adds that element to the Table path
"name" partition encodes the element in the Table name
"database" partition is attached to a field passed in that pulls out a different database connection
  (don't run to implement that)

The partition scheme can include ANY column from ANY table (intended principally for things like
Marea or Azone, but could also pull things out based on Bzone custom field tags for EJ zones or
whatever).

Elements not mentioned in the partition will be ignored. Elements mentioned in the partition but not
present in the specific table will also be ignored (i.e. not put in the path or name of the table).
Elements with "merge" action (or "none") will be treated as if they weren't there: they're just
making the default action explicit.

If we can figure out how to create a database with "pure SQL", we could add a "database" type to the
partition. The database is named after the model, with a timestamp. The $list function of the
exporter will help figure out who is where. So then we could shunt Global or whatever into a
different database.

To handle things like the Marea table, we can partition on "Global" in addition to "Year" (if we
don't do that, the Marea tables will be smashed together, and we'll use the "missing column"
mechanism to rewrite the table with new columns).

When we $write the table, we can add additional parameters like Global="Global" which add a column
with that value to the table if there is not already a column with that name. All the rules of
cbind'ing a column apply to that additional parameter (i.e. it can be a vector the same length as
the data.frame we're writing). Then that column can be used in the partition.

We should also allow a flag for "noPartition=TRUE" on $write, in which case we use Table name
verbatim and put it in the root of the connection (need to check for collision, and do an
"overwrite=TRUE" to drop and recreate the table; otherwise do nothing with an error message).

The VEPartition object has an $initialize function that sets up the partition scheme above.

VEPartition has a $partition function that we apply to a data.frame to figure out where and how it
will get partitioned for writing. We apply the partitioning scheme to the actual data and return a
list of partition tables into which to write subsets of data.frame rows. That list will have at
least one element (with Rows set to 1:nrow(data)), but may have more than one if the partition
includes fields like Azone or Marea that are not partitioned out during the VEResultsList $export
iteration.

To get the table location information for a data.frame via the Exporter $write function,)

  list(
    Path    = Vector (all the folder elements in order, named by the field used to select that path element)
    Name    = Vector (all the name elements from the partition in order, named by the field used to select that element)
    Table   = String (the base table name, always included by name and not part of the "Name" partitions)
    Rows    = vector of row numbers in the data.frame of what to write (default is ALL)
    Locator = String (mashup of Path+Name to determine uniqueness - essentially a hash to
                     used internally by the VEPartition to determine if we Append or Create)
  )

The exporter will track the Locators, and use that to determine whether to create or append to the
table. We want the exporter also track the fields available at that Location, so we can spot any
that are missing on one side or the other. 

The Exporter tracks existing Locators and builds the Location element into a table descriptor that
it uses to see if the table exists and what columns it has (we can cache that information). Then we
can rectify columns if the names in the database are not compatible with the names in the
data.frame. We will read and then rewrite the table if it is missing columns, and we will add
columns of NA to the new data.frame to make up any that are missing on that side.

    Action  = one of c("Append","Create")
    PathSeparator = Character # defined by the exporter class
    NameSeparator = Character # defined by the exporter class

How the external table path and name gets build depends on the Exporter, but the resolution order of
Path and Name elements depends on the order named in the original partitioning vector. The
VEExporter connection will furnish a (possibly configurable) strategy for assembling Table + Name (+
Path). Path elements will usually come first, followed by Name elements. The Table name can be
placed either at the front or at the end, or a template can be used to build them.

One question is whether to put a magic prefix on Scenario or Year or Global in the table name (if
included there) so we can parse what's a scenario or year or Azone or whatever.

So based on the above, here's the interface to VEExporter $write:

ve.exporter.write <- function(
  data,                  # a data.frame with partitionable columns
  Table,                 # character name for base table
  Metadata=NULL,         # If provided, a table with one row per field in data containing
                         # arbitrary fields of information about that Name (only Name is
                         # required). Name is supplemented with extra fields including
                         # S/G/T plus Units plus Description plus whatever from the field
                         # specs. Internally, an extra field will be added identifying
                         # (via Table Locator as returned by $list(details=FALSE)) where
                         # that S/G/T/N got written to. These get appended to the internal
                         # Location structure. Default Metadata will be just to dump list
                         # of tables and fields (which makes it uninteresting)
  Scenario=character(0), # name of scenario (1 element vector) to associate with this export
                         # exporter keeps a list so we can readily add new scenarios
                         # without having to filter them from the outside. Ignored if
                         # missing/empty, in which case table will always be
                         # written/appended (possibly leading to duplicate rows...
  overwrite=FALSE,       # if TRUE, delete existing table with same name; otherwise append rows
  noPartition=FALSE,     # if TRUE, ignore any partition attached to the connection and write bare Table name to root
  extraFields=list()     # named list of columns to cbind onto data before partitioning and writing
  ...                    # more named parameters added to extraFields
)

The VEExporter $list function returns a human readable version of the table descriptor
part of the locator. It reports back the locator information assembled across each call to
$write, so we have the full list of tables that were created. We basically want to cache
the location data from VEPartition, plus the assembled table name used by the specific
VEExporter implementation. It has this interface:

ve.exporter.list <- function(
  details = FALSE      # If TRUE, return a list of location structures for the Table
                       #   i.e. The full table location structure
                       # If FALSE, return a list of tables that were written (relative
                       #   to the connection root, so file.path(Path,paste(Names,table)
                       #   or some such for CSV or other file system hierarchical
                       #   storage.) The exporter will add that as an External element
                       #   to the structure that VEPartition$partition produced.
)

The VEExporter $data function iterates the internal set of tables and returns each as a
data.frame. It has this interface:

ve.exporter.data <- function(
  flatten = TRUE,      # if TRUE (default), generate a list of all the tables as data.frames
                       # if FALSE, turn each Path element into a list of elements with it
  attachTable = TRUE   # if TRUE (default), add Table name as an attribute to the data.store
                       # That will support re-exporting to a new partition scheme
  tables = list()      # list (or subset) equivalent to what you get by doing $list(details=TRUE) 
                       # permits not having to load entire export all at aonce
)

All of that should be enough to support copying an exported set of data to a (possibly)
different storage medium with a (possibly) different partitioning scheme.

So that brings us to the VEExporter $initialize.  Here's a proposed interface:

ve.exporter.initialize <- function(
  config = list(), # may have "Model" element for a VEModel to seek Exporter tag
                   # Model is optional (if missing, look just in global visioneval.cnf)
                   # plus "Tag" to seek a set of data within Exporter block.
                   # Tag is optional; if missing look for first of the tags associated
                   # with this VEExporter type. These will be used for connection defauls
                   # and only consulted for missing connection or partition parameters
                   # here. Only use if we don't have both connection and partition
                   # supplied explicitly here.
  data = NULL,     # if a named list of data.frames with Table attributes, write those out.
                   # if no Table attribute, use corresponding name of list element as
                   # a fixed table and don't partition. Can use this e.g. for queries or
                   # for re-partitioning a previous exports. If data is a VEExporter,
                   # then we'll get the list of data.frames by calling its $data function
                   # using its $list() function to load the other export one table at a
                   # time.
  connection=NULL, # A connection string or list that will be mined by the particular
                   # VEExporter subclass to set up a place to write tables. Will include
                   # things like table name component separators for the partition, path
                   # separators for CSV files, root directory within OutputDir for CSV
                   # or other file-based export. Generally a VEExporter subclass will
                   # wrap a "driver" (specific DBI + Driver, standalone Excel, etc.)
  partition=NULL,  # A partition descriptor used to build. a VEPartition processor. NULL
                   # says use the default from config parameter (or global configuration
                   # for driver default tag if config not supplied, or system default).
                   # To do no partitioning at all set partition=FALSE. In that case,
                   # any $write will just go straight to the Table in the connection
                   # root.
)

So we can create a "data" attribute to VEExporter $initialize which contains a list
of data.frames each of which has a Table attribute (basically, what gets returned from
$data), or perhaps contains another VEExporter, whose $data function will get called.
Default is an empty list, so nothing happens. If there is a list, we iterate it and
each data.frame + Table gets a $write called in the new exporter - effectively
re-partitioning the data.frames into the new exporter structure. Cool.

To do a Python-style yield, if we send an existing exporter into the constructor, it will
call the $list function and call the tables up one at a time to copy/partition the
results.

We also want to do a $save function on VEExporter. Then we can serialize the Exporter to a
NAME passed to $save and used by openExporter(load=NAME). The saved exporter gets a
.VEexport suffix and it's an R object dump (or perhaps YAML) of the exporter's tracking
information. We can optionally attach a time-stamp to the file name (and keep it in the
VEExporter class). The .VEexport object can be placed anywhere but defaults to root of
OutputDir

  timestamp (if any table locations)
  class/driver (could just be an exporter tag)
  connection (list of connection parameters like CSV root directory, DBI database, etc)
  partition
  table locations (dump of $list(details=TRUE))

So generally, functions that are writing stuff into an exporter will finalize/save when
they are done (thus VEResultsList$export and VEQuery$export). When we open an exporter
and write to it again, we won't write any S/G/T that already has a table location, unless
we do overwrite=TRUE in which case the connection root will be made "new" (with a new
timestamp). So the save protocol will only be an issue for someone who uses the exporter
directly (and forgets to $save). Could have an "interactive" flag that auto-saves after
a single $write (set to TRUE by default, but craftily changed by the interior calls).

For DBI databases, we should be able as part of the connection to create the database
there with an option to drop everything from an existing one, or create a new one from
a default name with name collision resolution (probably a timestamp). The ModelName should
get built into the database (along with any "database" elements of the partition - can we
create those on the fly?). The exporter should have all its entries timestamped once when
the exporter is first created, not when the object creation happens.

Then we can get away with minimum required credentials or noe 

So that would let us do this beautiful thing:

1. Run a model with sixteen scenarios
2. Define and run three more scenarios using "continue" (so earlier ones don't get re-run
3. Open an existing export and stock up the partitioning scheme and known locations
   Might require two tables: Partition and Locations (i.e. the existing tables)
4. Then we can just export the new scenarios to the existing export (need to figure
   out how not to rewrite any scenarios that were already done). So even though it's not
   necessary to the physical export, we might track Scenarios/ModelStages that were
   written to the export and skip any of those that were being re-done (an option
   on VEExporter $write. We don't associate the Scenario with specific tables, just
   note that it has been "processed").

Or we can use that loaded export to write to a different partitioning scheme on a
different connection / different VEExporter object - just visit each table in the $data of
the old scheme, and write it to the new exporter.

Probably want to do the exporter descriptor as a standalone structure (.VEexport file
format) which we can build, open, "run" on a particular model and generate that in the
results "output" directory so we can just pop open an export and work on it again in R.

That gives us an insanely convenient data explorer that we can use to both inspect, load
into data.frames, and re-export into a different file tree/database/internal data format
(any of the exporter types).

Whether or not we can $save depends on the VEExporter subclass. Only external storage
makes sense (can't save a data.frame exporter, but can copy it to a Database)

Saving an exporter only saves model, connection and partition information.

We can't write to a re-loaded exporter (if it has results, it's locked for further
writing - set a Locked flag internally when it's loaded don't let it be used in any
of the high-level export functions). It can be explored and copied, however.

Alternatively, which is much less fun, we don't allow a selection at all. The tables
will be carrying around everything we need to select (S/G/T/N). But on a big model,
that could get to be onerous. So here's the bite: allow an export to be added to, but
only if it's not a "selection" subset. If it was made from a selection, lock it.

That leads back into the Metadata architecture. Originally, I was thinking of Metadata
from the outside (that is, whatever selection was used to generate the export). But in
fact, what matters is the internal Location list the Exporter will build, listing each
table and its fields - so the Metadata is compiled by the Exporter as it writes things
out. The key thing we need to injects is the source S/G/T for each N added to a table,
and recognizing that the same field in the same table could have come from different
S/G (so we'll get another record onto that metadata when a different S/G contributes
a T/N, depending on the partitioning scheme). And we may be interested in additional
metadata fields for each S/G/T/N (at a minimum, Units and Description). So probably
what we need to write into the list of fields in the Location tracking is also a
data.frame with S/G/T/N/Metadata (one row per unique S/G/T/N) - For any table
we write into, the N will be unique and thus we can merge the destination table onto
the source metadata (passed in) using by=Name, and that resulting correspondence
table (with possibly more than one record per N) will go into the generated Metadata
to be placed in the export. Done.

Should be able to ask an Exporter for its partitioning scheme so we can borrow that into a
new exporter.

Queries are exported with an non-partitioned Exporter (Table just goes by name into the
connection root). So we'll use CSV or Excel exporter to generate output.

Could do the visualizer with a JSON exporter with its own root folder... We could then
make the visualizer file preparation into a special partitioning applied to Queries (and
one that expects to have additional attributes used to generate auxiliary tables for
categories, scenarios and data). All of those are just tables, and all of them can be
re-exported...
