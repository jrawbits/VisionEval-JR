Change how the queries manage their results.

Run ModelStage Punch List:
  - When we run a ModelStage, we need to save key features of the ModelStage into the ModelState.Rda
    In particular, we want to keep the Scenario/Level list. The RunParam_ls for the ModelStage
    (if it was built as a scenario) should include the "Scenario/Level" descriptors as
    built when we identify scenarios.

Setting up scenario ModelStages:
  - For manual scenarios, the ModelStage should include a "Category" key, referring to the
    "Categories" tag (which should only contain "NAME", "LABEL" and "DESCRIPTION", not "LEVELS").
    If there is no Category key, it gets a default one called "Scenarios".
  - If no "Scenarios" tag, but "Categories" exists, use that as below to classify any manual
    scenarios.

  - If no "Categories" and "Scenarios" are pre-defined:
      - "Categories" and "Scenarios" tags will be created as the set of unique "Category" tags in
        each Reportable ModelStage
      - Each reportable ModelStage:
        - Adds a Scenario/Level to the scenarios list (NAME=Scenario Name/Dir, LEVEL=1)
        - Adds a Category/Level (or just a level to the Categories table)
          - If the stage has no Category, its category is "Scenarios"
          - If the stage category does not exist it is created in the categories table
          - The stage is appended to the list of category levels (NAME = next available number)
          - The Category level only has one scenario (NAME=ModelStage name, LEVEL=1)

  - If there are "Categories" and "Scenarios" defined
      - "Categories" must contain "LEVELS", which is a list of the defined NAME and LEVEL from
        the "Scenarios" tag
      - The Scenario Name/Level is sought as an input folder in ScenarioDir (paste0(Name,Level))
        Input path for the Scenario/Level is the corresponding input folder. Description for
        the Scenario/Level is placed in the "Scenarios" tag.
      - We build model stages from combinations of Category/Level (where each Category/Level
        supplies the vector of InputPaths for its Scenario/Level elements as well as a vector
        of Levels for each Scenario associated with that Category).
          - Visit all the Scenario/Levels and add InputPath (normalized) and Level NAME
          - When building a ModelStage for a combination of Category/Level, add the InputPaths
            of all the Scenario/Levels in the Category/Levels and accumulate a named vector of
            Scenario/Level
          - A special "Level 0" is added to the front of the list of Category/Levels, with
            description "Base Conditions" and Name "0"
      - The StartFrom stage is also tagged: it provides the last element of InputPath for all
        the generated stages, and it is tagged with level zero for each Scenario entry.

Extracting information from ModelStages:
  - Need a function to generate the scenario descriptor for the visualizer's VEData output:
    "Scenario":<ScenarioDir> followed by the Scenario/Level (low-level, not categories) tags.

Query Punch List:

  - Change Geography not to include specific zones (GeoValue). Just include GeoType - and only
    process metrics compatible with that type.

  - Change the VEQuery $run function to deal exclusively with a VEModel or a VEResults, and
    drop support for any other kind of thing. We can either query a model or we can query one
    of more VEResults (implicitly a ModelStage), which just regenerates the query results for
    that stage.

  - Basic approach is to take a VEModel object, run its $query function supplying the path of
    a query (relative to modelPath/QueryDir or a full path) and an optional "QueryName" to use
    in building the output files. The result of that is a VEQuery object

  - From a VEquery, for which we can request $results (yields VEQueryResults) which comes from
    opening the Query's .RData in the associated model's ResultsDir.
  - A VEQuery object has a concept of an "attached VEModel"
  - Each VEModelStage also has a $query function that will work just on the stage, and that
    takes a VEQuery and returns the same VEQuery with Model/ModelStage attached.
  - When we have a VEQuery, we can run it (removes prior results in the attached model for that
    query, unless overwrite==FALSE in which case we report that it has already been run).
  - Requesting query results on a query that has not been run reports an error
    ("Query has not been run yet."). Likewise if no Model/ModelStage attached. Otherwise opens a
    VEQueryResults object
  - VEQueryResults will open the Query .RData in the ResultsDir of the attached Model/ModelStage
    (is a timestamp really needed?)
  - Can $clear VEQueryResults. Can $run VEQueryResults (which uses the specification saved
    in the .Rdata file).

  - Running just stores the raw values from SummarizeDatasets and saves them to the Query .Rdata.
    Results are always saved. Do not do a timestamp, but we can keep a Timestamp internally:
    Date on the VEQuery file versus Timestamp internally kept in QueryRun-<QueryName>.Rdata.
    Warn user if Timestamp on run is older.

  - A separate $export function will take the raw results and boil them down (including looking at
    just a subset of Geographies - we don't do that up front, though we do set the geography level).
    - Internally, we construct a list of one data.frame from the query by flattening the
      Values for the query results and then doing cbind to join it into the data.frame.
      - Values for an array value are flattened into a named vector that unrolls the dimension names
      - Option on the data.frame generation to include the Scenario/Level columns for visualizing
      - If exporting to data.frame or other tabular form, can filter by Category (all levels in
        that category will be added to the table with one column per Level) - that will also work
        for visualizing.
    - Then we have the option to
      - just return the data.frame
      - write the data.frame to a .csv file
      - write the "metadata" to a .VEqry file
      - write the "visualizer.js" file into the "data" subdirectory of a "visualizer" directory
        that gets created and populated with boilerplate inside ResultsDir. Also, visualizer.js
        in the export case includes the $(document).ready() method to invoke "VisualVE" javascript
        code.
    - Building the visualizer.js happens as follows:
        - calls a function in the VEModel to generate the scenario description JSON elements
          - categoryconfig
          - scenarioconfig
          - Those two files will essentially be a JSON dump of what is maintained internally.
          - Pseudo-files will be created if a single ModelStage/VEResults is being visualized
        - calls a function in the VEQueryResults to generate the output configuration
          - outputconfig
          - Dumps the QuerySpecification:
            "NAME": "DVMT Per Capita",
            "LABEL": "Daily Vehicle Miles Traveled",
            "DESCRIPTION": "daily vehicle miles of travel of residents divided by population.",
            "INSTRUCTIONS": "daily vehicle miles of travel of residents divided by population.",
            "METRIC": "Average",
            "UNIT": "daily miles",
            "COLUMN": "DVMTPerCapita"
        - Constructs the VEData JSON object using a function in VEQueryResults that visits each
          ModelStage and gets its (including the StartFrom stage to create the data table). Creates a
          named list with the ModelStage name; one column for each Scenario, showing the level; and
          one column for each metric and its value)

   - The $visualize function works just like exporting to "visualize" format, except that it starts
     a JRC webpage and pushes out the base page (with all its JS and CSS components findable
     straight from the package version of the page), then it formates visualizer.js (in memory)
     and uses SendCommand to push visualizer.js. In that case it doesn't do the $(document).ready
     command, but instead just makes a direct call to VisualVE.
     - For JRC management, we keep the web socket as a field in the VEModel. If there is something
       already in that field, we start by closing it and then recreate everything from the ground
       up.

Don't use the .VEqry suffix for the query output .Rdata - just make it an .Rdata file.

In the .Rdata file, The QuerySpecification list for the Query is joined by a "Value" object which
contains either a scalar, named vector or named matrix/array (convention for which is the first
index of the array - see existing code). When the query is run, the results are ALWAYS placed in
ResultsDir with the file name: QueryRun-<QueryName>.RData. So we end up with:
  - QuerySpecification
  - Value (scalar, vector or array, with names)
  - Dimension (what kind of thing is the vector; what is in each dimension of the the array)
    All of which is conventional (I think it's geography first, then "Breaks/By")

So here's the summary of running:
  - for a VEModelStage, write just its Values+QuerySpec+Dimension into an .RData file in the
    ResultsDir/StageDir output location.
  - for a VEModel, develop a list of Reportable stages, identify whether we are doing discrete
    scenarios or combinations, write that result into the Model ResultsDir (root) then visit each
    ModelStage and run the query on that stage (generating its results).
  - Key technical consideration is to load the ModelState for the stage and use its Datastore
    path to find necessary data elements.

We can subset the visualization for a model by listing just specific categories to include - as
if those were all that were configured. If we export (instead of visualize) a result subset, we
get a data.frame/csv/sql for all the levels withing those categories (missing categories are
treated as if we asked for "Level 0"). Or do we filter Scenarios, which are the actual files...

For VEResults, also improve the export: format can be "csv", "data.frame" (return), or "sql" (where
the SQL database DBI connection is provided). The .csv export also produces a metadata file
indicating what was selected for export, as is currently the case, and the results go into a
timestamped sub-folder.

So the only difference in exporting VEResults versus exporting a .VEqry is how the data.frame list
is generated. We can ask for .VEqry metadata, which does a text dump of the results .Rdata file.

Internally, VEResults and VEModel can use the same mechanism for generating their output. We start
by generating data.frame(s) (a list if more than one), then use the output formatter to generate
the return. If we ask for data.frames, they are returned visibly. If we ask for some other format,
we get an invisible named list of data.frames (with a special class VEExport that tells "print" to
list their names not their values) - probably do that via a class-specific "as.character" function
(and an as.data.frame function which just removes the "VEExport" class).

If we ask for .csv (either exporting a query or extracted tables) then the data.frames are written
out to a series of time-stamp folders containing .csv files named after the data.frames. Overwriting
is not an issue for .csv since they will just keep piling up in OutputDir.

If we ask for SQL and provide a connection to a database, the data.frames are written to tables (the
names are the data.frame names, only SQL-ized), and if those tables already exist they are first
dropped (if overwrite=TRUE) or the transaction is cancelled with an error message (if
overwrite=FALSE). For SQL, if instead of a dbiconnecton we provide a filename, a text .SQL file will
be generated in a SQL dump format (dropping and recreating tables, and providing INSERT statements
to load the data).

Need to update the "dir" and "clear" functions for the model to manage queries. QueryDir contains
query templates. Need to recognize queries in the ResultsDir and treat those like log files with
respect to listing and clearing. Clearing results will take out the QueryRun-*.Rda along with Logs.

