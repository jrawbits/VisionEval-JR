Break the Query run into layers like the model run: we accumulate the list of reportable stages at
the overall level, generate a Values structure (all the metrics) for each one of them, and save
those Values into the QueryResults file for the stage. Then at the Model level, we're also
accumulating the Values into a list by stage. Further simplification: don't save the stage query
results - just stick all of them into the model level results. Obviously, when we run the query,
we should be able to give a list of stages (to override the default Reportable ones).

The Category/Level structure for a generated scenario should be retained in the scenario's
ModelStage RunParams_ls so we have it ready to generate the visualizer files.

The query will do all breaks and all geographies. Only one Sub-regional geography can be specified
(Azone or Bzone). but we can filter the metrics to only include certain zones.

And we can filter a subset of the metrics or a subset of the stages after the query is run.

When we run a query, we generate VEQueryResults for each VEResults object and save that out into the
VEResults output path (query and date). Done like that, we can just save the query spec and add a
"Value" structure into the list. The "Value" structure is just a nested list with a single numeric
value (named after the Metric) if it is scalar, with a named numeric vector if there is either a set
of zones or a set of breaks, and a named array if there is a set of zones AND a set of breaks. We'll
flatten the array or vector into multiple rows when exporting into CSV or data.frame or when
visualizing.

It would be nice to be able to subset VEQueryResults. Get a list of metric fields defined in the
results and then provide a vector of names to create a VEQueryResult with just the named metrics
in it (reduce both the QuerySpecification and the Results list). We have to give the extract a name
(it will also get a new Timestamp) and it gets saved. Likewise, we may also wish to limit to 
a smaller set of ModelStages.

A VEQueryResults is a sub-class of VEQuery. VEQueryResults can handle two cases: the results for
running the query on a specific set of VEResults (a model stage), or the results for running the
query on a model (with a set of model stages). Perhaps they could just be one form: the list of
ModelStages may have length 1, and there is the OPTIONAL Values list, which has the same number of
elements as the number of Stages. Typically, it will only be present for results associated with a
single model stage, but that gives us the option to package the entire set of results in one single
.Rda file that we can reopen ("openQueryResults") and then work on, even if the model and
directories are not available. That's one way of bundling the results in a quasi-portable form.

And a VEQueryResults differs from VEQuery in that it has ModelStage names, Timestamp, and Values.
Probably also want to track what VEModel it's associated with (and the model's ResultsDir and
each ModelStage's StageDir). We need the VEModel in order to access ScenarioCategory information -
or that information (the Categories and Levels in particular, with their ScenarioGroup element)
could also be included alongside the ModelStages.

For the single model stage, the results are stored in a named list of Value elements, which will be
a named numeric vector (length 1 or more) or a named numeric array (with 2 dimensions). The
VEQueryResults also has a Timestamp corresponding to when the query was run (a conventional time
from calling VEQuery$run).

For the set of model stages (full model with direct stages or scenarios), the VEQueryResults stores
a list of ModelStages - their Name and RunPath - and the Timestamp (no Values - those are acquired
indirectly by visiting each of the ModelStages.

When a Query is run, the Query results are saved in the ResultsDir (for a Model with scenarios) or
in the ResultsDir/StageDir (for a single model stage). The results are always saved in a file called
Query-<QueryName>-Timestamp.Rda (an RData file).

We can ask a VEModel for its query results (list of all available results) or a list of its known
queries (which may or may not have results). Or we can ask a VEQuery for its results given a
particular model (or run it on that model). We can open Query results directly by providing the file
path (which implies a RunPath, a ModelState and thus a Model).

With VEQueryResults, we have two options: export(data.frame return, CSV file, Tableau CSV(s),
Visualizer) or visualize (function which uses JRC to launch the Visualizer). Exports go into
OutputDir for the Stage or Model the VEQueryResults are associated with. If we export the
visualizer then the system will copy the HTML tree and sub-directories to an OutputDir sub-directory
named Query-<QueryName>-Visualizer-<Timestamp>, and inject a script command into the end of the HTML
body that will load the visualizer.js file. The .js could include "VisualVE" function call at the
end after the required data has been defined - so it's just one line of Javascript that we could
insert through a template substitution into the same HTML used by the JRC "live" visualizer.

The Visualizer HTML should show the Timestamp for the query run that generated the results that are being
displayed.

So then we can just use the SummarizeDatasets at a very low level to inject the results directly
into the QuerySpec, which we can then save out (along with metadata indicating that this QuerySpec
contains run results). That result obeject will be written into
VEResults$Path/OutputDir/QueryName-TimeStamp.veqry. If we load a VEQry that has run results, it just
loads as a query. Any editing will remove all the results (and we can re-save it). If we visit
multiple results, their query results all get the same TimeStamp, and the model and its scenarios
get (in the Model's ResultsDir/OutputDir) a timestamp for the query run (present in all stages that
completed). The .veqry output in that case lists ModelStage Results that were processed by the query
run (by ModelStage Dir) as well as the QuerySpec. So if we load the Query object in the model
results, it will assemble all the stage results.

Need to ponder the management a bit: a QuerySpec is rather model-independent. a QueryRun (in
OutputDir/queries) contains the QuerySpec used to generate the results, plus the results themselves.
The results for a single stage are a "Value" element in the QuerySpec list structure itself - and
we probably want to do a "save.image" with a list of objects and have the results stored as .Rdata
files. The results for a set of scenarios contains the key elements of the scenario setup: the
model stages (and their directory locations), the Timestamp identifying the Query Run, and the
Categories and Levels if they are available.

From there, we can export into a variety of formats:
  1. (export) Query metadata (row = metric, columns = metadata)
  2. (export) Query values as CSV (row=flattened metric (zones,breaks), column=value)
  3. (export) Query values as Tableau format (possibly multiple files, key structure)
  4. (export) Visualizer (writes an entire local HTML hierarchy like VEScenario did)
  5. (visualize) Query values as Visualizer javascript (single file defining Javascript data objects for
     VisualVE plus function call to force the page). Visualizer uses a different base page
     depending on whether it is looking at the VEQuery result for
     a. one scenario
     b. a set of manual (ModelStage or folder) scenarios, or
     c. a set of Category/Level scenarios

Could just distribute the query results (as QueryName-Timestamp) directly into the Stage's RunPath.
And if we run for a model (with or without scenarios), we generate a QueryName-Timestamp in the
modelPath directory. Don't bother putting queries into the OutputDir - save that just for exports.
So the Model's QueryName-Timestamp will contain references to the stages that were queried so we can
look in the stage sub-folders to get the actual values (found using the QueryName and Timestamp in
each stage's results). Either use a Query- prefix or a special extension to mark these as query
results. Then if we export the query as CSV or Tableau or Visualizer, it goes into OutputDir for
the associated stage or model. We have a framework function to Visualize such a file (or we can
apply the $visualize function to an unexported VEQueryResults object loaded from a time-stamped
run).

If we have a VEQuery open, we can give it a model and either run the Query again or report on
existing runs in the Model's ResultsDir (and even report if it's out of date by looking at the
model last run timestamp compared to query results timestamp) and then open one of those.

Once we have the VEQueryResults, we can export or visualize. Very nice workflow!

When handling scenario groups - if no scenario group is listed for a category, the category is
promoted to a one-category scenario group.

So running a query on a VEResults object creates a VEQueryResults which contains a reference to the
VEQuery (which defines the metrics that get generated) and a reference to the VEResults object used
to generate the data. So the basic query on the results

If we run a Query on a model with VEModelScenarios, then all the Reportable stages in the model are
processed (we get one VEQueryResults for each stage) and the run returns a VEQueryScenarios object
which contains a list of VEQueryResults and some helper functions (to load/save, export (CSV,
Tableau), visualize). Initially, we can only visualize Category scenarios (not ModelStages or Folder
based).

Choosing visualize will generate the (scenario-cfg.js) ScenarioGroups (culled from
Categories/Levels) as well as (category-cfg.js) Categories and Levels and (output-cfg.js) which
reports the metrics that will be present in the VE data (vedata.js). We'll still produce the
reference values, but those don't appear to be used.

If we're NOT doing Category scenarios: We could just generate one bar chart with an on/off column
for each individual scenario. The output metrics show one column per scenario and the Y-Axis has
the numeric results (so the output charts are rotated 90 degrees) so we can visually compare the
results from each scenario on each metric.

To drive the visualizer, we can use JRC. The page loads everything - JS, CSS, Images - using the
document root pointing into the package. That includes VisualVE as a function (it doesn't run when
the page is loaded). Then after the page is up, we follow withSendCommand to send Javascript that
will create the configuration and data objects. Then we reset the page (clear the input and output
charts) and call the VisualVE function which operates on the global variables we just used
Javascript commands to create. The function rebuilds all the elements and redraws the page. Ta-Da!
That should be pretty easy!

==== TECHNICAL VIEWER NOTES

The VisionEval viewer is built on three interrelated Javascript libraries:

Javascript => is an operator for creating a lambda function (without its own "this"):

"s => s.length" is results-wise the same as "function(s){ return s.length }"

D3-JS (https://d3js.org)

D3 allows you to bind arbitrary data to a Document Object Model (DOM),
and then apply data-driven transformations to the document. For
example, you can use D3 to generate an HTML table from an array of
numbers. Or, use the same data to create an interactive SVG bar chart
with smooth transitions and interaction.

Readers familiar with other DOM frameworks such as jQuery should
immediately recognize similarities with D3. Yet styles, attributes,
and other properties can be specified as functions of data in D3, not
just simple constants. Despite their apparent simplicity, these
functions can be surprisingly powerful; the d3.geoPath function, for
example, projects geographic coordinates into SVG path data. D3
provides many built-in reusable functions and function factories, such
as graphical primitives for area, line and pie charts.

Crossfilter.js (http://crossfilter.github.io/crossfilter/)

Crossfilter is a JavaScript library for exploring large multivariate
datasets in the browser. Crossfilter supports extremely fast (<30ms)
interaction with coordinated views, even with datasets containing a
million or more records.

The coordinated visualizations below (built with D3)...

dc.js (https://dc-js.github.io/dc.js/)

dc.js is a javascript charting library with native crossfilter
support, allowing highly efficient exploration on large
multi-dimensional datasets (inspired by crossfilter's demo). It
leverages d3 to render charts in CSS-friendly SVG format. Charts
rendered using dc.js are data driven and reactive and therefore
provide instant feedback to user interaction.